{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4p5h+AZ+LRzquSvG/AE5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayanmitra2021/CUDA_Practice/blob/master/Cuda_practice_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyyE9kmgk8th",
        "outputId": "84905108-5807-40a1-b8ac-f10ea17278c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqtdcFyclP2U",
        "outputId": "1f6f9358-28e0-4361-c63a-b19e4ba14e37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpni3nrae3\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// A helper function to check for CUDA errors\n",
        "void checkCudaError(cudaError_t err, const char* message) {\n",
        "    if (err != cudaSuccess) {\n",
        "        fprintf(stderr, \"CUDA Error: %s - %s\\n\", message, cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void helloFromGPU() {\n",
        "    printf(\"Hello World from GPU thread %d!\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    std::cout << \"Hello World from CPU!\" << std::endl;\n",
        "    helloFromGPU<<<1, 5>>>();\n",
        "    checkCudaError(cudaGetLastError(), \"Kernel Launch Failed\");\n",
        "    checkCudaError(cudaDeviceSynchronize(), \"cudaDeviceSynchronize Failed\");\n",
        "    std::cout << \"\\nSuccessfully synchronized with GPU.\" << std::endl;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkejJ_PIlZep",
        "outputId": "e135b5bf-dee9-43ea-ca86-0bf5760c892e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc test.cu -o test_executable -arch=sm_75"
      ],
      "metadata": {
        "id": "w0VkV5ofpcij"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./test_executable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3rgJvJupgMn",
        "outputId": "2cdcbba8-8758-4f85-e6c7-14d8837dccac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from CPU!\n",
            "Hello World from GPU thread 0!\n",
            "Hello World from GPU thread 1!\n",
            "Hello World from GPU thread 2!\n",
            "Hello World from GPU thread 3!\n",
            "Hello World from GPU thread 4!\n",
            "\n",
            "Successfully synchronized with GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "\n",
        "// This kernel will add a value to each element of an array on the GPU\n",
        "__global__ void add(int *a, int value) {\n",
        "    a[threadIdx.x] += value;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 5;\n",
        "    int host_a[N] = {10, 20, 30, 40, 50}; // Data on the CPU\n",
        "    int *device_a; // Pointer for data on the GPU\n",
        "\n",
        "    // 1. Allocate memory on the GPU\n",
        "    cudaMalloc(&device_a, N * sizeof(int));\n",
        "\n",
        "    // 2. Copy data from CPU to GPU\n",
        "    cudaMemcpy(device_a, host_a, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    std::cout << \"Data on CPU before kernel launch:\" << std::endl;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        std::cout << host_a[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // 3. Launch the kernel on the GPU to add 100 to each element\n",
        "    add<<<1, N>>>(device_a, 100);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 4. Copy the modified data back from GPU to CPU\n",
        "    cudaMemcpy(host_a, device_a, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 5. Free memory on the GPU\n",
        "    cudaFree(device_a);\n",
        "\n",
        "    // 6. Print the result from the CPU\n",
        "    std::cout << \"\\nData on CPU after kernel launch:\" << std::endl;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        std::cout << host_a[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKxijmCkptWo",
        "outputId": "476336f4-e4a9-4da9-f6f2-f839866aec78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data on CPU before kernel launch:\n",
            "10 20 30 40 50 \n",
            "\n",
            "Data on CPU after kernel launch:\n",
            "10 20 30 40 50 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}